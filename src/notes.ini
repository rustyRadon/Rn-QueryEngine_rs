Performance Goals: Minimize "pointer chasing" (indirection) and eliminate heap allocations during the execution phase.

Foundation,"Query engine basics, Apache Arrow, Type Systems, Data Sources."
Representation,"Logical Plans, DataFrame API, SQL Support."
Execution,"Physical Plans, Query Planning, Joins, Subqueries."
Optimization,"Query Optimizer rules, Execution performance."
Scaling,"Parallel Execution (Cores), Distributed Execution (Clusters)."
Quality,"Testing strategies (Fuzzing), Benchmarking."

...KEY
Term,Simple Explanation
Memory Allocation,"Asking the computer for a ""desk"" to put your data on. Too much ""asking"" makes the engine slow."
The Stack,"A small, incredibly fast ""workbench"" for immediate tasks."
The Heap,"A giant ""warehouse"" for storing large amounts of data (like your columns)."
SIMD,"""Single Instruction, Multiple Data."" A way to tell the CPU ""Add these 8 pairs of numbers at the exact same time"" instead of one by one."

Module	            Description
datatypes	    Type system built on Apache Arrow
datasource	    Data source abstractions and CSV/Parquet readers
logical-plan	Logical plans and expressions
physical-plan	Physical plans and expression evaluation
query-planner	Translation from logical to physical plans
optimizer	    Query optimization rules
sql	            SQL tokenizer, parser, and planner
execution	Q   uery execution engine
examples	    Example queries and benchmarks
benchmarks	    Benchmarking utilities and query benchmarks.

                                    ANATOMY OF THE QUERY ENGINE
use std::sync::Arc;

/// 1. DATA TYPES (The Foundation)
/// Using an enum for the Type System allows for static dispatch during execution.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum DataType {
    UInt32,
    Float64,
    Utf8,
}

/// 2. LOGICAL PLAN (The "What")
/// A recursive tree structure representing the user's intent.
/// We use Arc (Atomic Reference Count) because plans are often shared across threads.
#[derive(Debug)]
pub enum LogicalPlan {
    /// Scanning a data source like a CSV or Parquet file
    Scan { path: String, schema: Vec<(String, DataType)> },
    /// Filtering rows based on a condition
    Filter { input: Arc<LogicalPlan>, expression: String },
    /// Selecting specific columns
    Projection { input: Arc<LogicalPlan>, columns: Vec<String> },
}

/// 3. PHYSICAL PLAN (The "How")
/// This trait defines the contract for executable code.
/// It is 'Sync' because it must be safe to share across CPU cores.
/// Send + Sync, meaning it can be moved between threads safely.
pub trait PhysicalOperator: Send + Sync + std::fmt::Debug {
    /// Executes the operator and produces a stream of data.
    /// In a real engine, this returns a Stream of RecordBatches.
    fn execute(&self) -> Result<(), EngineError>;
}

#[derive(Debug)]
pub enum EngineError {
    IoError(String),
    Internal(String),
}

/// 4. THE QUERY ENGINE (The Orchestrator)
pub struct QueryEngine {
    // We avoid 'mut' here because the engine configuration should be immutable
    // once initialized, ensuring thread safety without locks.
    version: &'static str,
}

impl QueryEngine {
    pub fn new() -> Self {
        Self { version: "0.1.0" }
    }

    /// The "Planning" stage: Translating 'What' to 'How'
    /// We borrow the LogicalPlan to avoid expensive clones of the plan tree.
    pub fn create_physical_plan(
        &self, 
        logical_plan: &LogicalPlan
    ) -> Arc<dyn PhysicalOperator> {
        // In a real engine, this involves a 'match' statement walking the tree.
        // We return an Arc<dyn Trait> to allow for a heterogenous tree of operators.
        match logical_plan {
            LogicalPlan::Scan { .. } => Arc::new(EmptyExec),
            _ => todo!("Implement other operators"),
        }
    }
}

#[derive(Debug)]
struct EmptyExec;
impl PhysicalOperator for EmptyExec {
    fn execute(&self) -> Result<(), EngineError> {
        Ok(())
    }
} 

// IDIOMATIC TRANSFORMATION:
        // 1. Check if empty
        // 2. If NOT empty, wrap in Int32Column and put in Some()
        // 3. Otherwise, return None
        // 4. Wrap the whole thing in Ok()
        Ok((!batch_data.is_empty()).then_some(Int32Column { data: batch_data }))


////////////////---------------/////////////////
            NOTE ONTHE THREE
////////////////---------------/////////////////

// ============================================================================
// PROJECT: Minimalist Vectorized Query Engine (V3 - Production Style)
// ROLE: Principal Rust Systems Engineer
// ============================================================================

use std::fs::File;
use std::io::{BufReader, Read};
use std::sync::{Arc, Mutex};
// EXTERNAL CRATE: byteorder is the standard for binary decoding.
use byteorder::{LittleEndian, ReadBytesExt};

/// Represents the physical data types supported by our engine's memory model.
/// We use `Arc<Vec<T>>` to ensure that column data is stored once on the heap
/// and shared via reference counting.
#[derive(Debug, Clone)]
pub enum Column {
    /// 32-bit Signed Integer column
    Int32(Arc<Vec<i32>>),
    /// 64-bit Floating Point column
    Float64(Arc<Vec<f64>>),
}

/// A horizontal slice of a table. In high-performance engines, data is processed
/// in "batches" (usually 1024-4096 rows) to balance CPU cache usage and overhead.
#[derive(Debug, Clone)]
pub struct RecordBatch {
    /// Contiguous memory blocks for each column
    pub columns: Vec<Column>,
    /// Metadata: The names of the columns (e.g., "age", "salary")
    pub names: Vec<String>,
}

/// The core trait for all physical operators. 
/// Every node in the query plan must implement this to produce data.
pub trait ExecutionTask: Send + Sync {
    /// Retrieves the next chunk of data from the stream.
    /// Returns `Ok(None)` to signal "End of Stream" (EOS).
    fn next_batch(&self) -> Result<Option<RecordBatch>, String>;
}

// ----------------------------------------------------------------------------
// OPERATOR: SCAN
// ----------------------------------------------------------------------------

/// Reads raw binary data from a file and transforms it into a `RecordBatch`.
pub struct ScanWorker {
    /// Mutex is required because `BufReader`'s internal cursor changes on every read.
    /// In a multi-threaded context, we must synchronize this "seek" operation.
    pub file_handle: Mutex<BufReader<File>>,
    pub column_name: String,
}

/// --- OPERATOR: SCAN ---
pub struct ScanWorker {
    pub file_handle: Mutex<BufReader<File>>,
    pub column_name: String,
}

impl ExecutionTask for ScanWorker {
    fn next_batch(&self) -> Result<Option<RecordBatch>, String> {
        // 1. LOCKING: Lock the file to prevent race conditions on the file cursor.
        let mut guard = self.file_handle.lock()
            .map_err(|_| "Lock poisoned".to_string())?;

        // 2. PRE-ALLOCATION: Justify capacity of 1024 to minimize heap 'growth' events.
        let mut batch_data = Vec::with_capacity(1024);

        // 3. READ LOOP: Leveraging byteorder for precise numeric extraction.
        for _ in 0..1024 {
            match guard.read_i32::<LittleEndian>() {
                // Success: Value is pushed to the heap-allocated vector.
                Ok(value) => batch_data.push(value),

                // Clean Termination: We hit the end of the file perfectly.
                Err(e) if e.kind() == ErrorKind::UnexpectedEof => break,

                // System Failure: IO error (e.g. disk failure) must be reported, not ignored.
                Err(e) => return Err(e.to_string()),
            }
        }

        // 4. PIPELINE RETURN: Use functional check to avoid empty batch propagation.
        Ok((!batch_data.is_empty()).then_some(RecordBatch {
            columns: vec![Column::Int32(Arc::new(batch_data))],
            names: vec![self.column_name.clone()],
        }))
    }
}

// ----------------------------------------------------------------------------
// OPERATOR: FILTER
// ----------------------------------------------------------------------------

/// Filters rows based on a boolean predicate.
pub struct FilterWorker {
    /// The upstream operator (could be a Scan, another Filter, etc.)
    pub input: Arc<dyn ExecutionTask>,
    /// A pointer to the logic used to evaluate each row.
    pub predicate: fn(i32) -> bool,
}

impl ExecutionTask for FilterWorker {
    fn next_batch(&self) -> Result<Option<RecordBatch>, String> {
        // RECURSIVE PULL: Ask the upstream worker for data.
        let input_batch = self.input.next_batch()?;

        // FUNCTIONAL PIPELINE: Transform the Option without 'if let' branching.
        Ok(input_batch.and_then(|batch| {
            // TYPE SAFETY: We match on the column type.
            match &batch.columns[0] {
                Column::Int32(data) => {
                    // ITERATOR PATTERN: Highly optimized by LLVM; often auto-vectorized.
                    let filtered: Vec<i32> = data.iter()
                        .filter(|&&v| (self.predicate)(v))
                        .copied() // copied() is a zero-cost way to handle i32
                        .collect();

                    // CONDITIONAL RETURN: If no rows pass, this batch effectively disappears.
                    (!filtered.is_empty()).then_some(RecordBatch {
                        columns: vec![Column::Int32(Arc::new(filtered))],
                        names: batch.names,
                    })
                },
                // If the column type doesn't match our predicate, we return None.
                _ => None, 
            }
        }))
    }
}

// ----------------------------------------------------------------------------
// OPERATOR: PROJECTION
// ----------------------------------------------------------------------------

/// Selects a subset of columns from the input batch.
pub struct ProjectionWorker {
    pub input: Arc<dyn ExecutionTask>,
    /// Indices of the columns to retain (e.g., [0, 2] to keep 1st and 3rd columns).
    pub projection_indices: Vec<usize>,
}

impl ExecutionTask for ProjectionWorker {
    fn next_batch(&self) -> Result<Option<RecordBatch>, String> {
        // MAP PATTERN: If upstream gives data, slice it. Otherwise, propagate None.
        Ok(self.input.next_batch()?.map(|batch| {
            // UNZIP PATTERN: Efficiently builds two vectors in a single pass.
            let (cols, names): (Vec<_>, Vec<_>) = self.projection_indices.iter()
                .map(|&i| (batch.columns[i].clone(), batch.names[i].clone()))
                .unzip();

            RecordBatch { columns: cols, names }
        }))
    }
}

////////////////---------------/////////////////
            NOTE ON THE THRE END
////////////////---------------/////////////////

Row ID,Name (String),Age (Int32),Salary (Int32)
1    ,Alice,        15,             0
2    ,Bob           20,            500
3    ,Charlie,      25,            2000
4    ,David,        30,            3500
5    ,Eve,          35,            5000
6    ,Frank         40,            7000

/////////////////---------------/////////////////
        TEN PERFORMANCE TUNING TIPS
/////////////////---------------/////////////////
1. Enable SIMD (Single Instruction, Multiple Data)
Right now, your filter checks ages one by one. Modern CPUs have instructions that can check 8 or 16 integers at the exact same time in a single clock cycle.

The Fix: Use crates like std::simd or packed_simd to process "chunks" of the filter mask simultaneously.

2. Multi-Threading (Parallel Scanning)
You are currently scanning age.bin and salary.bin on a single CPU thread.

The Fix: Use Rayon to scan both files in parallel or split the files into "shards" (partitions) so 4 or 8 CPU cores can filter different parts of the data at the same time.

3. Avoid String Column Names in Batches
Your RecordBatch likely carries a Vec<String> for column names. Comparing strings or cloning them during every next_batch call is slow.

The Fix: Use "Interned Strings" or simple usize IDs to identify columns inside the pipeline.

4. Zero-Copy Deserialization
Currently, you read bytes into a buffer and then copy them into a Vec<i32>.

The Fix: Use Memory Mapping (mmap). This tells the OS to map the file directly into your program's memory space. You can "cast" the file bytes directly into a slice &[i32] without copying a single byte.

5. Bit-Packed Filter Masks
Your filter mask is a Vec<bool>. In Rust, a bool takes 1 entire byte (8 bits).

The Fix: Use a BitSet (or Roaring Bitmap). You can store 8 "Keep/Toss" decisions in a single byte, making the mask 8x smaller and much faster for the CPU to read.

6. Columnar Compression (Dictionary Encoding)
If you have a "Department" column with only 3 values (Sales, HR, Engineering) repeated 1 million times, storing the strings is wasteful.

The Free Fix: Store them as integers (0, 1, 2) and only translate them to strings at the very end when printing.

7. Fixed-Size Batching
If your batches are too small (e.g., 10 rows), the "function call overhead" kills performance. If they are too big (e.g., 1 million rows), they don't fit in the CPU L1 Cache.

The Fix: Optimize batch sizes to be exactly 1024 or 4096 rows. This is the "sweet spot" for modern CPU caches.

8. Predicate Pushdown
Right now, you "Zip" everything and THEN "Filter." If the filter fails, you wasted time zipping the salary.

The Fix: Push the filter all the way down to the ScanWorker. Only read the Salary from the disk if the Age already passed the test. This saves massive amounts of Disk I/O.

9. Loop Unrolling & Branch Prediction
If your code has a lot of if statements inside the loops, the CPU gets "confused" (Branch Misprediction).

The Fix: Write the code in a way that the CPU can predict the next step easily, or use #[inline(always)] hints to help the compiler optimize the assembly code.

10. Memory Pool Allocation
Allocating new Vecs for every batch causes the "Global Allocator" to slow down.

The Fix: Use a Buffer Pool. Re-use the same memory buffers for every batch instead of throwing them away and asking the OS for new ones.

